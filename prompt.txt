Voici une description complète et structurée du projet "2Long2Read", conçue pour vous permettre de générer tous les documents nécessaires (présentation, documentation, etc.).1. Contexte du ProjetDans le cadre de notre cours "Automation & Deployment", nous devons concevoir et déployer une infrastructure de données complète pour une entreprise fictive1. Notre équipe a choisi de créer un service public et utile : un analyseur automatisé de Conditions Générales d'Utilisation (CGU).Le problème : Les CGU sont des contrats juridiques que nous acceptons tous les jours sans les lire. Rédigées dans un jargon complexe, elles peuvent contenir des clauses qui portent atteinte à la vie privée, cèdent des droits de propriété sur nos contenus, ou autorisent des pratiques commerciales abusives.Notre solution : Le projet "Lisez-moi si vous osez" est une plateforme web qui permet à n'importe qui de soumettre l'URL d'une page de CGU. Notre infrastructure analyse alors le texte, identifie les clauses potentiellement dangereuses, et restitue un rapport simple et compréhensible. Nous agissons comme un "traducteur" technique et juridique pour le grand public, redonnant aux utilisateurs le pouvoir sur leurs données.2. ObjectifsLes objectifs du projet sont alignés sur les exigences de l'évaluation 2.Objectif 1 : Analyser les contraintes. Nous devons gérer des contraintes techniques (sites web avec JavaScript), opérationnelles (analyser de nombreuses URL en parallèle), et réglementaires (le contexte du RGPD est au cœur de notre sujet)3.Objectif 2 : Concevoir une architecture adaptée. Nous allons designer une architecture microservices robuste, scalable et sécurisée, capable de répondre aux besoins du service4.Objectif 3 : Déployer l'infrastructure. L'ensemble de nos services sera conteneurisé et déployé sur un environnement cloud via Kubernetes5.Objectif 4 : Orchestrer et monitorer. Nous utiliserons Airflow pour automatiser le pipeline d'analyse et une stack Prometheus/Grafana pour superviser la performance et la santé de notre infrastructure en temps réel6666.Objectif 5 : Documenter et présenter. Le projet final inclura une documentation technique complète, une vidéo de démonstration et une soutenance orale devant un jury7.3. Outils et TechnologiesNos choix d'outils sont motivés par la pertinence technique et les standards de l'industrie, tout en intégrant des innovations justifiées.OutilCatégorieRôle dans le projetDockerConteneurisationIsoler chaque service (API, Scraper, Analyseur) dans son propre conteneur pour un déploiement fiable et reproductible.KubernetesOrchestrationGérer le cycle de vie de nos conteneurs : déploiement, mise à l'échelle automatique (scalabilité) et haute disponibilité.AirflowAutomatisationOrchestrer le pipeline d'analyse en plusieurs étapes pour chaque URL soumise, de la collecte du texte à la génération du rapport final.PlaywrightScraping WebPiloter un navigateur headless pour extraire le contenu textuel des CGU, même sur des sites complexes utilisant du JavaScript.PythonLangage BackendDévelopper nos microservices avec des frameworks comme FastAPI pour l'API et des librairies d'analyse de texte (NLTK, spaCy).MongoDBBase de DonnéesStocker les résultats de nos analyses dans un format de document flexible (JSON), parfaitement adapté à la nature semi-structurée des clauses extraites.PrometheusMonitoring (Collecte)Collecter activement des métriques détaillées sur la performance de nos services directement depuis Kubernetes.GrafanaMonitoring (Visualisation)Créer des tableaux de bord pour visualiser les métriques collectées par Prometheus et surveiller la santé de l'infrastructure en temps réel.GitHub ActionsCI/CDAutomatiser les tests, la construction de nos images Docker et leur publication, garantissant la qualité et la fiabilité de notre code.4. ArchitectureNous adoptons une architecture microservices où chaque composant a une responsabilité unique. Cela rend le système plus facile à maintenir, à mettre à jour et à faire évoluer.Frontend (Interface Utilisateur) : Une simple page web où l'utilisateur soumet une URL.API Gateway (Service API) : Le point d'entrée unique. Il reçoit la requête et la valide.Broker de Messages (ex: RabbitMQ) : L'API ne lance pas directement l'analyse. Elle dépose une "demande de travail" dans une file d'attente pour découpler les services.Orchestrateur (Airflow) : Airflow écoute cette file d'attente. Dès qu'une nouvelle demande arrive, il déclenche le pipeline d'analyse.Workers (Services d'Analyse) : Airflow lance des jobs Kubernetes à la demande :Un Worker de Scraping (avec Playwright) récupère le texte.Plusieurs Workers d'Analyse (avec des scripts Python) traitent le texte en parallèle.Base de Données (MongoDB) : Les workers stockent leurs résultats dans la base.Monitoring (Prometheus & Grafana) : Prometheus collecte des métriques sur tous les composants, et Grafana les affiche.5. Logique du Pipeline d'Analyse (Le "Cerveau")La logique principale est orchestrée par un DAG Airflow qui se déroule comme suit pour chaque URL :start_scraping : Un premier pod est lancé avec Playwright. Il navigue vers l'URL, attend que tout le contenu soit chargé, et extrait le texte brut. Le texte est sauvegardé dans un stockage temporaire.prepare_analysis : Le texte est nettoyé (suppression du HTML, des sauts de ligne inutiles) et divisé en paragraphes ou en phrases.run_parallel_analyzers : C'est le cœur du système. Airflow lance plusieurs tâches en parallèle. Chacune est un pod différent qui exécute un script d'analyse spécifique :Analyseur de "Partage de Données" : Recherche des mots-clés comme "partager", "vendre", "partenaires", "tiers", "données personnelles".Analyseur de "Propriété Intellectuelle" : Recherche "licence", "propriété", "droit d'auteur", "cédez", "contenu utilisateur".Analyseur de "Résiliation" : Recherche "résilier", "suspendre", "sans préavis", "à notre seule discrétion".Analyseur de "Lisibilité" : Calcule des scores de complexité du texte (ex: indice de Flesch-Kincaid).consolidate_results : Une fois toutes les analyses parallèles terminées, une dernière tâche récupère tous les résultats partiels.generate_final_report : Cette tâche synthétise les informations, calcule le score de dangerosité global (par exemple, en additionnant les points de chaque clause à risque trouvée) et enregistre le rapport final au format JSON dans la base de données MongoDB.end_and_notify : Le statut de l'analyse est mis à jour comme "terminé" dans la base. Le frontend peut maintenant afficher le rapport à l'utilisateur.
