apiVersion: v1
kind: PersistentVolume
metadata:
  name: airflow-dags-pv
  namespace: airflow
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteMany
  hostPath:
    path: "/tmp/airflow-dags"
    type: DirectoryOrCreate
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: airflow-dags-pvc
  namespace: airflow
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-dags-sync
  namespace: airflow
data:
  sync-dags.sh: |
    #!/bin/bash
    # Simple script to copy DAG to mounted volume
    echo "Syncing DAG file..."
    mkdir -p /opt/airflow/dags
    cat > /opt/airflow/dags/cgu_analysis_dag.py << 'DAGEOF'
    """
    DAG Airflow pour l'analyse de Terms & Conditions.
    Uses KubernetesPodOperator - the proper way to run pods from Airflow!
    """
    import pendulum
    from airflow.models.dag import DAG
    from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator
    from airflow.operators.bash import BashOperator
    from kubernetes.client import models as k8s

    with DAG(
        dag_id="cgu_analysis_pipeline",
        start_date=pendulum.datetime(2025, 1, 1, tz="UTC"),
        catchup=False,
        schedule=None,
        tags=["2long2read", "tc-analysis"],
        description="Analyse des Terms & Conditions avec Claude AI",
    ) as dag:

        # Task 1: Run analysis using KubernetesPodOperator
        run_analysis = KubernetesPodOperator(
            task_id="run_cgu_analysis",
            name="cgu-analysis-worker",
            namespace="airflow",
            image="2long2read-worker:latest",
            image_pull_policy="IfNotPresent",
            cmds=["python3"],
            arguments=[
                "/app/worker.py",
                "--task-id", "{{ dag_run.conf.get('task_id', 'manual-ui-trigger-' + ts_nodash) }}",
                "--source-name", "{{ dag_run.conf.get('source_name', 'manual_ui_test') }}",
                "--text-content", "{{ dag_run.conf.get('text_content', 'TEST TERMS AND CONDITIONS: This is a minimal test document used when triggering from Airflow UI. By using this service, you agree to our terms. We collect your data. We may terminate your account at any time. You waive all legal rights. This is only a test to verify the pipeline works correctly.')[:50000] }}",
            ],
            env_vars={
                "MONGO_HOSTNAME": "mongo-service.default.svc.cluster.local",
                "MONGO_PORT": "27017",
                "ANTHROPIC_API_KEY": "{{ var.value.ANTHROPIC_API_KEY }}",
            },
            get_logs=True,
            is_delete_operator_pod=False,
            in_cluster=True,
            startup_timeout_seconds=600,
        )

        # Task 2: Log completion
        log_completion = BashOperator(
            task_id="log_completion",
            bash_command="""
            echo "========================================="
            echo "CGU Analysis Pipeline Completed"
            echo "Task ID: {{ dag_run.conf.get('task_id', 'unknown-task') }}"
            echo "Source: {{ dag_run.conf.get('source_name', 'unknown') }}"
            echo "========================================="
            """,
        )

        run_analysis >> log_completion
    DAGEOF
    echo "DAG file synced successfully"
    ls -la /opt/airflow/dags/
